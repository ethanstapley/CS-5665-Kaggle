{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ae73bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T21:19:01.175913Z",
     "iopub.status.busy": "2025-04-20T21:19:01.175649Z",
     "iopub.status.idle": "2025-04-20T21:19:10.620941Z",
     "shell.execute_reply": "2025-04-20T21:19:10.620156Z"
    },
    "papermill": {
     "duration": 9.449425,
     "end_time": "2025-04-20T21:19:10.622334",
     "exception": false,
     "start_time": "2025-04-20T21:19:01.172909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "test_df = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n",
    "texts = test_df[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4181643",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T21:19:10.627133Z",
     "iopub.status.busy": "2025-04-20T21:19:10.626797Z",
     "iopub.status.idle": "2025-04-20T21:19:16.824995Z",
     "shell.execute_reply": "2025-04-20T21:19:16.824160Z"
    },
    "papermill": {
     "duration": 6.202147,
     "end_time": "2025-04-20T21:19:16.826489",
     "exception": false,
     "start_time": "2025-04-20T21:19:10.624342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the tf-idf + linear regression implementation\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Custom text features\n",
    "class TextFeatures(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.swear_list = {\"fuck\", \"shit\", \"bitch\", \"damn\", \"ass\", \"crap\", \"dick\"}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        features = pd.DataFrame()\n",
    "        features[\"text_length\"] = X.apply(len)\n",
    "        features[\"word_count\"] = X.apply(lambda x: len(x.split()))\n",
    "        features[\"punct_count\"] = X.apply(lambda x: sum(1 for char in x if char in \"!?.,;:\"))\n",
    "        features[\"uppercase_ratio\"] = X.apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1))\n",
    "        features[\"swear_word_count\"] = X.apply(lambda x: sum(1 for word in x.lower().split() if word in self.swear_list))\n",
    "        return features.values\n",
    "\n",
    "# Load and clean training data\n",
    "train_df = pd.read_csv(\"/kaggle/input/train-ranking/train.csv\")\n",
    "train_df[\"clean_text\"] = train_df[\"comment_text\"].apply(clean_text)\n",
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "weights = [.32, 1.82, .16, 1.5, .64, 1.5]\n",
    "train_df[\"severity\"] = train_df[label_cols].dot(weights)\n",
    "\n",
    "# Balance dataset\n",
    "toxic_df = train_df[train_df[\"severity\"] > 0]\n",
    "non_toxic_df = train_df[train_df[\"severity\"] == 0].sample(n=len(toxic_df), random_state=42)\n",
    "balanced_df = pd.concat([toxic_df, non_toxic_df]).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Build final pipeline with fixed best params\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    min_df=3,\n",
    "    max_df=0.75,\n",
    "    ngram_range=(1,1)\n",
    ")\n",
    "\n",
    "combined_features = FeatureUnion([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"extra_features\", TextFeatures())\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"features\", combined_features),\n",
    "    (\"linear\", LinearRegression(fit_intercept=False))\n",
    "])\n",
    "\n",
    "pipeline.fit(balanced_df[\"clean_text\"], balanced_df[\"severity\"])\n",
    "\n",
    "# Predict on test set\n",
    "test_df = pd.read_csv(\"/kaggle/input/jigsaw-toxic-severity-rating/comments_to_score.csv\")\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(clean_text)\n",
    "linear_scores = pipeline.predict(test_df[\"clean_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74aabab1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T21:19:16.830787Z",
     "iopub.status.busy": "2025-04-20T21:19:16.830556Z",
     "iopub.status.idle": "2025-04-20T21:21:16.892492Z",
     "shell.execute_reply": "2025-04-20T21:21:16.891315Z"
    },
    "papermill": {
     "duration": 120.066252,
     "end_time": "2025-04-20T21:21:16.894577",
     "exception": false,
     "start_time": "2025-04-20T21:19:16.828325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 21:19:23.823346: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745183964.016295      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745183964.072438      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Using the BERT Summation implementation\n",
    "\n",
    "model_path = \"/kaggle/input/bert-toxic-epoch1/other/default/1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "bert = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "bert.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_bert_sum(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
    "    inputs = {k: v.to(bert.device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        logits = bert(**inputs).logits\n",
    "        probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n",
    "    weights = [.32, 1.82, .16, 1.5, .64, 1.5]\n",
    "    return np.dot(probs, weights)\n",
    "\n",
    "bert_scores = [get_bert_sum(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "364684df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T21:21:16.900908Z",
     "iopub.status.busy": "2025-04-20T21:21:16.899669Z",
     "iopub.status.idle": "2025-04-20T21:21:16.905423Z",
     "shell.execute_reply": "2025-04-20T21:21:16.904960Z"
    },
    "papermill": {
     "duration": 0.009309,
     "end_time": "2025-04-20T21:21:16.906425",
     "exception": false,
     "start_time": "2025-04-20T21:21:16.897116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df[\"score\"] = (linear_scores + bert_scores) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bb2c312",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T21:21:16.910626Z",
     "iopub.status.busy": "2025-04-20T21:21:16.910188Z",
     "iopub.status.idle": "2025-04-20T21:21:16.937999Z",
     "shell.execute_reply": "2025-04-20T21:21:16.937269Z"
    },
    "papermill": {
     "duration": 0.031042,
     "end_time": "2025-04-20T21:21:16.939131",
     "exception": false,
     "start_time": "2025-04-20T21:21:16.908089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission complete!\n"
     ]
    }
   ],
   "source": [
    "submission = test_df[[\"comment_id\", \"score\"]]\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission complete!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 3445671,
     "sourceId": 27935,
     "sourceType": "competition"
    },
    {
     "datasetId": 7102506,
     "sourceId": 11350721,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7203634,
     "sourceId": 11491683,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 311729,
     "modelInstanceId": 291040,
     "sourceId": 348510,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 143.24765,
   "end_time": "2025-04-20T21:21:20.407354",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-20T21:18:57.159704",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
